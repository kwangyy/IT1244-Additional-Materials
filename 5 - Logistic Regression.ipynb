{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Materials Part 5: Logistic Regression\n",
    "\n",
    "We will tackle the third ML model learnt in IT1244 called Logistic Regression. This is a new model for classification problems, but this lesson will be a LOT more than that.\n",
    "\n",
    "I will be introducing a lot more stuff here, so do take time to digest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-load some stuff, mainly some datasets from scikitlearn \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "water = pd.read_csv(\"data/water_cleaned.csv\", index_col =\"Unnamed: 0\")\n",
    "\n",
    "# Let's split the data into X and y \n",
    "X = water.drop(\"Potability\", axis = 1)\n",
    "y = water[\"Potability\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Initialisation of models (and a lil more)\n",
    "\n",
    "By this part, you should have learnt how to manipulate data in Pandas (with the other supplementary materials) or with NumPy. \n",
    "\n",
    "As a recap, the workflow of models usually goes like this:\n",
    "1. Find the model from sklearn - it's usually in a separate library\n",
    "2. do a train test split on the data (80/20? 70/30? up to you)\n",
    "3. fit the data onto the training data\n",
    "4. predict the results using the test data \n",
    "5. compare the predictions against the y values of test data\n",
    "\n",
    "How do we apply this for Logistic Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6276703967446592"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K-Nearest Neighbours is taken from sklearn.linear_model\n",
    "# train_test_Ssplit is taken from skmodel.model_selection\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# We will then split the data into different datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Save the model into a variable \n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the data onto training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict results using test data\n",
    "predictions = logreg.predict(X_test)\n",
    "\n",
    "# Let's get the accuracy score to see how it does! \n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this is like 7.9% better than what we did for KNN, but this begs the question..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.5: Is there anything we can do better? \n",
    "\n",
    "We're always looking to do better, right? Would you trust someone that makes their decision 62% of the time? Surely there are things that we can do to improve the accuracy of our model.\n",
    "\n",
    "This is where I introduce to you the Standard Scaler - it is a scaler that scales your numerical features using standardization (as covered in lectures). \n",
    "\n",
    "!!! Important: you __need__ to scale your data after you have split your dataset. This is to prevent data leakage and make sure that your model can adapt well to unseen data!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6286876907426246"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see where we're supposed to fit our StandardScaler\n",
    "\n",
    "# First, let's import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# We fit our scaler onto the data here as data preprocessing. \n",
    "# Fit and transform the scaler on the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize logreg \n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict results using test data\n",
    "predictions = logreg.predict(X_test_scaled)\n",
    "\n",
    "# Let's get the accuracy score to see how it does! \n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh, okay. there's a 0.1% increase. Honestly, it's not much, but it is always good to scale your features so that it gives you more accurate results.\n",
    "\n",
    "The elephant in the room, however, is that there it's still performing terribly, which makes you wonder..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Wait, does this even matter?\n",
    "\n",
    "In lesson 3, we mentioned that accuracy is the simplest metric to implement, but it is not the best metric to use. You might be wondering why!\n",
    "\n",
    "### A hypothetical situation...\n",
    "Let's imagine we have a dataset of 100 datapoints to train a model to predict whether or not a patient has cancer. Out of those 100 datapoints, 99 datapoints classifies \"No cancer\" while 1 datapoint classifies \"cancer\".\n",
    "\n",
    "What if I told you that the model has a 99% accuracy - would you think that it's good? But what if I told you that it consistently predicts \"no cancer\" well but it cannot predict \"cancer\" well? \n",
    "\n",
    "This is why we accuracy isn't the best metric to use - we have to consider a LOT of things for this! It is a lot better to use other metrics for this. This is where a classification report comes in!\n",
    "\n",
    "### Classification report\n",
    "Let's look at what a classification report does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77       617\n",
      "           1       1.00      0.00      0.01       366\n",
      "\n",
      "    accuracy                           0.63       983\n",
      "   macro avg       0.81      0.50      0.39       983\n",
      "weighted avg       0.77      0.63      0.49       983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, there are a whole ton of metrics here:\n",
    "\n",
    "1. Precision\n",
    "2. Recall\n",
    "3. F1-score\n",
    "4. Support\n",
    "\n",
    "Let's talk about the definition of these metrics:\n",
    "\n",
    "1. Precision - basically the number of true positives / total positives (true and false positives). This measures how accurate your model is when it predicts a positive class.\n",
    "2. Recall - number of true positives / true positives + false negatives. This measures how accurate your model is when it predicts an actual positive class.\n",
    "3. F1-score - the harmonic mean of precision and recall, defined as 2 * Precision * Recall / (Precision + Recall). If you want a balance between precision and recall, this is the metric you're using.\n",
    "4. Support - the number of actual occurrences of the class in the testing dataset.\n",
    "\n",
    "### What should I be focusing on then?\n",
    "It really depends on your use case. \n",
    "\n",
    "If you're looking for your models to have lesser false positives (it is predicted as positive even though really it's negative), you're looking to increase precision. \n",
    "For example, if you're working on a fraud detection model and your emphasis is on maintaining a seamless experience for legitimate customers while still catching the most obvious fraudulent cases, you are more tolerant to false negatives and you'd like to decrease as many false positives as possible. \n",
    "\n",
    "\n",
    "If you're looking for your models to have lesser false negatives (it is predicted as negative even though really it's positive), you're looking to increase recall. For example, if you're training a model that predicts customers that will stop subscribing to a subscription-based service, it's better to have a false positive and to classify as \"unsubscriber\" even though they might not actually be unsubscribing. You'd like to decrease as many false negatives as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Hyperparameter Tuning\n",
    "\n",
    "For most models, there will be hyperparameters that you can tune. This is a lot more applicable to the optional lesson that you can access, as there are a lot more parameters there! \n",
    "\n",
    "Let me introduce you to two that you will often try:\n",
    "1. Learning Rate / C - this is how fast your model learns from the data. If the learning rate is too small, it will take ages for the model to learn from the data. However, if the learning rate is too big, it might cause the model to diverge instead, which is not a good thing.\n",
    "2. Max iterations - this is the number of times you iterate on the learning. Adjusting this will help especially if you find your model to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 0.001}, best score: 0.6022680785074825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# We're not using maxiter for this one, but you can still use it! \n",
    "param_grid = {\n",
    "    # \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, n_jobs = 1)\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "best_params = logreg_cv.best_params_\n",
    "best_score = logreg_cv.best_score_\n",
    "\n",
    "# EXERCISE: Repeat the same for RandomizedSearchCV\n",
    "\n",
    "print(f\"Best parameters: {best_params}, best score: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we have tuned hyperparameters, the model still doesn't seem to be doing well... \n",
    "\n",
    "It could be that the data wasn't great in the first place, or that we might need a model that is more accurate (in exchange for interpretability). Read up 5.5 for more details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Further reading\n",
    "\n",
    "Here are some things I have covered in this lesson:\n",
    "\n",
    "1. [Here's something on data leakage.](https://www.kaggle.com/code/alexisbcook/data-leakage)\n",
    "2. [More into the GridSearchCV vs RandomizedSearchCV difference!](https://datascience.stackexchange.com/questions/63129/gridsearchcv-vs-randomsearchcv-and-how-it-works)\n",
    "3. [Looking to understand why we need to trade accuracy for interpretability/flexibility?](https://www.baeldung.com/cs/ml-flexible-and-inflexible-models)\n",
    "\n",
    "For the next part, we will talk about additional models that you can use - but I will only give a superficial explanation of what the models are. Don't use these models unless you get a better understanding of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
